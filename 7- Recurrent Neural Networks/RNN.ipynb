{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 82s 5us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "input_train shape: (25000, 500)\n",
      "input_test shape: (25000, 500)\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 147s 466ms/step - loss: 0.6757 - accuracy: 0.5760 - val_loss: 0.6410 - val_accuracy: 0.6322\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 144s 459ms/step - loss: 0.6123 - accuracy: 0.6758 - val_loss: 0.6122 - val_accuracy: 0.6470\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 139s 444ms/step - loss: 0.4953 - accuracy: 0.7710 - val_loss: 0.5729 - val_accuracy: 0.7168\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 144s 460ms/step - loss: 0.3532 - accuracy: 0.8449 - val_loss: 0.4629 - val_accuracy: 0.7960\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 146s 468ms/step - loss: 0.2465 - accuracy: 0.9025 - val_loss: 0.4792 - val_accuracy: 0.7922\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 143s 458ms/step - loss: 0.1732 - accuracy: 0.9381 - val_loss: 0.5411 - val_accuracy: 0.7872\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.5316 - accuracy: 0.7879\n",
      "Test Loss: 0.5316097140312195 - Test Accuracy: 0.7879199981689453\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Constants\n",
    "VOCAB_SIZE = 10000\n",
    "MAXLEN = 500\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(\"Loading data...\")\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=MAXLEN)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=MAXLEN)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)\n",
    "\n",
    "def build_model(rnn_cell):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "        rnn_cell(32),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "rnn_model = build_model(tf.keras.layers.SimpleRNN)\n",
    "\n",
    "lstm_model = build_model(tf.keras.layers.LSTM)\n",
    "\n",
    "gru_model = build_model(tf.keras.layers.GRU)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "history = rnn_model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[callback])\n",
    "\n",
    "results = rnn_model.evaluate(input_test, y_test)\n",
    "print(f'Test Loss: {results[0]} - Test Accuracy: {results[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 8s 19ms/step - loss: 0.4904 - accuracy: 0.7605 - val_loss: 0.3512 - val_accuracy: 0.8536\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.2600 - accuracy: 0.8992 - val_loss: 0.2961 - val_accuracy: 0.8786\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 5s 18ms/step - loss: 0.1923 - accuracy: 0.9327 - val_loss: 0.3361 - val_accuracy: 0.8524\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.1487 - accuracy: 0.9481 - val_loss: 0.3389 - val_accuracy: 0.8576\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3502 - accuracy: 0.8548\n",
      "Test Loss: 0.3501717448234558 - Test Accuracy: 0.8547599911689758\n"
     ]
    }
   ],
   "source": [
    "history_LSTM = lstm_model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[callback])\n",
    "\n",
    "results_LSTM = lstm_model.evaluate(input_test, y_test)\n",
    "print(f'Test Loss: {results_LSTM[0]} - Test Accuracy: {results_LSTM[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 7s 19ms/step - loss: 0.4882 - accuracy: 0.7487 - val_loss: 0.3499 - val_accuracy: 0.8558\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.2601 - accuracy: 0.8963 - val_loss: 0.3019 - val_accuracy: 0.8746\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.1848 - accuracy: 0.9317 - val_loss: 0.3161 - val_accuracy: 0.8792\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.1420 - accuracy: 0.9504 - val_loss: 0.3442 - val_accuracy: 0.8760\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.3708 - accuracy: 0.8645\n",
      "Test Loss: 0.37075895071029663 - Test Accuracy: 0.8645200133323669\n"
     ]
    }
   ],
   "source": [
    "history_GRU = gru_model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[callback])\n",
    "\n",
    "results_GRU = gru_model.evaluate(input_test, y_test)\n",
    "print(f'Test Loss: {results_GRU[0]} - Test Accuracy: {results_GRU[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, 500, 32)      320000      ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  (None, 500, 32)      8320        ['embedding_9[0][0]']            \n",
      "                                                                                                  \n",
      " attention_6 (Attention)        (None, 500, 32)      0           ['lstm_7[0][0]',                 \n",
      "                                                                  'lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 16000)        0           ['attention_6[0][0]']            \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            16001       ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 344,321\n",
      "Trainable params: 344,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "VOCAB_SIZE = 10000 \n",
    "MAXLEN = 500  \n",
    "\n",
    "def build_model2(rnn_cell, rnn_units=32):\n",
    "    sequence_input = Input(shape=(MAXLEN,), dtype='int32')\n",
    "    embedded_sequences = Embedding(VOCAB_SIZE, rnn_units)(sequence_input)\n",
    "    rnn_output = rnn_cell(rnn_units, return_sequences=True)(embedded_sequences)\n",
    "    \n",
    "    query_value_attention_seq = tf.keras.layers.Attention()([rnn_output, rnn_output])\n",
    "    \n",
    "    attention_flatten = Flatten()(query_value_attention_seq)\n",
    "\n",
    "    dense_output = Dense(1, activation='sigmoid')(attention_flatten)\n",
    "    \n",
    "    model = Model(inputs=sequence_input, outputs=dense_output)\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# LSTM Model with Attention\n",
    "lstm_model_with_attention = build_model2(tf.keras.layers.LSTM)\n",
    "lstm_model_with_attention.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 10s 27ms/step - loss: 0.4309 - accuracy: 0.7860 - val_loss: 0.3505 - val_accuracy: 0.8540\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.2237 - accuracy: 0.9115 - val_loss: 0.3671 - val_accuracy: 0.8456\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.1579 - accuracy: 0.9392 - val_loss: 0.3504 - val_accuracy: 0.8838\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.1078 - accuracy: 0.9582 - val_loss: 0.4100 - val_accuracy: 0.8732\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.0756 - accuracy: 0.9730 - val_loss: 0.5851 - val_accuracy: 0.8750\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.6136 - accuracy: 0.8587\n",
      "Test Loss: 0.6136218309402466 - Test Accuracy: 0.8587200045585632\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_ = lstm_model_with_attention.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[callback])\n",
    "\n",
    "results_ = lstm_model_with_attention.evaluate(input_test, y_test)\n",
    "print(f'Test Loss: {results_[0]} - Test Accuracy: {results_[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
